[00:00:21.252 --> 00:00:22.973]  Welcome to the Infowars Nightly News.
[00:00:23.173 --> 00:00:23.874]  I'm David Knight.
[00:00:23.994 --> 00:00:26.716]  It's Thursday, September 12, 2013.
[00:00:27.436 --> 00:00:30.578]  Now tonight, we're going to look at the American regime's credibility.
[00:00:31.079 --> 00:00:32.780]  It's an absolute free fall.
[00:00:33.400 --> 00:00:44.528]  We've got the former leader of the KGB lecturing America about the rule of law and trying to stop the American regime from expanding the Middle East war into possibly a world war.
[00:00:44.848 --> 00:00:48.991]  We've got the former leader of the CIA being jeered and heckled
[00:00:49.471 --> 00:00:55.875]  We've got Harry Reid wringing his hands about independent-minded congressmen calling them anarchists.
[00:00:56.495 --> 00:01:06.060]  And, although it's hardly news, we have yet another official being caught in a bold-faced lie, committing perjury to try to cover up criminal activity.
[00:01:06.300 --> 00:01:07.301]  That's with the IRS.
[00:01:07.841 --> 00:01:15.265]  Now, earlier today on the Alex Jones Radio Show, although we're not going to play it here on the Nightly News, Alex talked to the widow of Ibrahim Todeshev.
[00:01:16.006 --> 00:01:16.686]  Now, if you remember,
[00:01:17.563 --> 00:01:24.809]  Ibrahim Todesha was taken into custody by the FBI to question him because he happened to know one of the patsies in the Boston bombing.
[00:01:25.790 --> 00:01:31.015]  He was shot in the heart three times, once in the liver and then in the back of the head.
[00:01:31.615 --> 00:01:33.317]  It was nothing but an execution.
[00:01:33.377 --> 00:01:35.559]  Clearly, from the wounds, it was an execution.
[00:01:36.604 --> 00:01:39.446]  This is the kind of regime we have in America now.
[00:01:39.486 --> 00:01:42.128]  This is the kind of criminals who are running our government.
[00:01:42.348 --> 00:01:45.911]  This is what we expect from a banana republic, an authoritarian regime.
[00:01:46.532 --> 00:01:58.100]  Don't tell me that if the FBI would do this to somebody, and everybody knows that they killed this guy when they took him into questioning, don't tell me that they wouldn't kill a high-profile journalist and try to make it look like an accident.
[00:01:58.681 --> 00:02:05.086]  They tried to shut him up, they tried to intimidate other people, but all it's done is make us more angry.
[00:02:06.343 --> 00:02:09.306]  We've got people who are sick and tired of the lies.
[00:02:09.746 --> 00:02:11.448]  We're sick and tired of the corruption.
[00:02:11.628 --> 00:02:14.190]  We're sick and tired of the spying and the endless war.
[00:02:14.531 --> 00:02:15.972]  And this is what it looks like.
[00:02:29.997 --> 00:02:33.201]  Now those are college students, but this is not like the Vietnam War.
[00:02:33.722 --> 00:02:37.067]  We've got middle-aged, middle-class people protesting the war.
[00:02:37.588 --> 00:02:45.158]  We've got Democrats, Republicans, Occupy Wall Streeters, Tea Party people who are protesting the banker bailouts and the NSA.
[00:02:45.959 --> 00:03:02.889]  We've got congressmen speaking out on both sides of the aisle, and we've got small businesses and unions who are angry about the service sector part-time economy and poverty that's being forced on us with Obamacare and other kinds of regulations coming from this regime.
[00:03:03.770 --> 00:03:05.891]  Look at what Harry Reid had to say.
[00:03:06.011 --> 00:03:07.312]  He's very upset.
[00:03:07.572 --> 00:03:10.674]  He's very upset about what's happening in Congress.
[00:03:11.314 --> 00:03:13.276]  He said, the anarchists have taken over.
[00:03:14.261 --> 00:03:17.165]  They've taken over in the House, now they've taken over in the Senate.
[00:03:17.485 --> 00:03:22.772]  People who don't believe in government, and that's what the Tea Party's all about, he said, are winning, and that's a shame.
[00:03:23.353 --> 00:03:26.818]  No, Harry Reid is wrong about that like he's wrong about so many things.
[00:03:26.878 --> 00:03:28.860]  The American people are not anti-government.
[00:03:29.301 --> 00:03:31.043]  They're anti-authoritarian government.
[00:03:31.063 --> 00:03:32.685]  They're anti-totalitarian government.
[00:03:33.065 --> 00:03:36.009]  They want a constitutional republic that respects their rights.
[00:03:36.509 --> 00:03:38.091]  And they're angry about the U.S.
[00:03:38.131 --> 00:03:39.492]  government supporting Al-Qaeda.
[00:03:39.933 --> 00:03:43.197]  We've got people like Dennis Kucinich, former Democrat congressman.
[00:03:43.457 --> 00:03:47.662]  We've got a Republican senator, Ted Cruz, both talking about how the U.S.
[00:03:47.822 --> 00:03:49.323]  is Al-Qaeda's air force.
[00:03:49.964 --> 00:03:59.757]  Everyone knows that American government is supporting Al Qaeda at the same time they're using it as an excuse to take away our long-fought civil liberties.
[00:04:03.888 --> 00:04:06.569]  I'm Jakari Jackson with an InfoWars Nightly News Alert.
[00:04:06.789 --> 00:04:08.810]  We have a new article on InfoWars.com.
[00:04:09.271 --> 00:04:12.552]  CIA begins delivering weapons to Al-Qaeda in Syria.
[00:04:12.993 --> 00:04:21.897]  The United States has officially announced it is now delivering lethal aid to the rebels engaged in attacks against the Syrian government, according to the Washington Post.
[00:04:22.377 --> 00:04:29.001]  The Post reports shipments were previously stalled due to logistical challenges involved in delivering equipment in a war zone.
[00:04:29.501 --> 00:04:33.603]  And officials fears that any assistance could wind up in the hands of jihadists.
[00:04:34.544 --> 00:04:37.565]  It's true that some of Assad's opponents are extremists.
[00:04:39.026 --> 00:04:48.992]  But Al Qaeda will only draw strength in a more chaotic Syria if people there see the world doing nothing to prevent innocent civilians from being gassed to death.
[00:04:50.753 --> 00:04:54.255]  The majority of the Syrian people and the Syrian opposition we work with
[00:04:55.103 --> 00:04:58.344]  Just want to live in peace, with dignity and freedom.
[00:04:59.105 --> 00:05:06.187]  So that was President Obama admitting that some of Assad's opposition are extremists, such as the Syrian rebels he provides weapons to.
[00:05:06.588 --> 00:05:12.370]  But in the defense of the President, he did say he would pursue political solutions after he's finished bombing Syria.
[00:05:12.850 --> 00:05:14.531]  Meanwhile, back here in the States, we have this.
[00:05:15.071 --> 00:05:18.072]  Felon charged with gun show sale to Austin officer.
[00:05:18.432 --> 00:05:23.634]  Prosecutors on Wednesday announced the arrest on a charge of being a felon in possession of a firearm.
[00:05:23.835 --> 00:05:29.977]  Rodriguez on August 18th allegedly sold a pistol to an undercover officer for $650.
[00:05:30.417 --> 00:05:37.760]  A search warrant carried out last week at the suspect's home led to the seizure of 76 firearms including assault rifles and also about $15,000.
[00:05:39.781 --> 00:05:46.384]  The article goes on to point out that Rodriguez had previously been in prison for the sale of firearms and also being in possession of a machine gun.
[00:05:46.624 --> 00:05:50.585]  So here in the States, if you exercise your Second Amendment rights, you get sent to prison.
[00:05:50.906 --> 00:05:55.708]  But if you arm Syrian rebels who behead Christians and burn churches, you're a Peace Prize winner.
[00:05:55.968 --> 00:05:59.109]  I'm Jakari Jackson and this has been an InfoWars Nightly News Alert.
[00:06:01.547 --> 00:06:09.648]  That's right, we can't expect the government to respect our rights and our liberties when they don't respect the lives and the rights of people abroad.
[00:06:10.268 --> 00:06:17.070]  This is a criminal regime, and it's not just going after made-up enemies in foreign countries like it did in Vietnam.
[00:06:17.150 --> 00:06:20.750]  No, they're coming after us now as well, and people understand that.
[00:06:21.150 --> 00:06:23.131]  Look at this quote from Bill Gertz.
[00:06:23.531 --> 00:06:24.291]  He quoted a U.S.
[00:06:24.351 --> 00:06:30.752]  official as stating, we've come full circle from going after Al-Qaeda to indirectly backing Al-Qaeda.
[00:06:31.292 --> 00:06:34.315]  Everyone knows that's true, except perhaps Rachel Maddow.
[00:06:34.796 --> 00:06:35.997]  She still hasn't gotten the memo.
[00:06:36.557 --> 00:06:44.344]  Everyone else on both sides of the aisle, both parties, the entire public, understands that we are now engaging and supporting Al Qaeda.
[00:06:44.384 --> 00:06:47.607]  At the same time, that's the excuse to take away our civil liberties.
[00:06:48.496 --> 00:06:52.038]  Look at what Putin said in his emergency warning to America.
[00:06:52.058 --> 00:06:56.261]  Now certainly I don't agree with a lot of what he said about the United Nations.
[00:06:56.641 --> 00:06:59.223]  I don't think that we should surrender our sovereignty to them.
[00:06:59.703 --> 00:07:01.765]  I don't think they're the ultimate legal authority.
[00:07:02.145 --> 00:07:06.208]  But he had a lot of things that were very true that he was saying about America.
[00:07:06.288 --> 00:07:07.749]  And everyone respects the fact
[00:07:08.289 --> 00:07:22.134]  That he's being upfront and talking about things in a way to try to create peace and hopefully we can have peace and not make the war broader, not prolong it, not expand it into a broader war throughout the Middle East.
[00:07:22.434 --> 00:07:24.095]  This is what he says about false flags.
[00:07:24.135 --> 00:07:36.019]  He says, no one doubts that poison gas was used in Syria, but there's every reason to believe that it was not used by the Syrian army, but by opposition forces to provoke intervention by their powerful foreign patrons.
[00:07:36.539 --> 00:07:38.361]  Who would be siding with the fundamentalists?
[00:07:38.601 --> 00:07:39.382]  That would be us.
[00:07:39.542 --> 00:07:40.763]  That would be the American government.
[00:07:41.063 --> 00:07:44.146]  He then went on to talk about weapons of mass destruction and their proliferation.
[00:07:44.426 --> 00:07:50.091]  He said the world reacts by asking, if you can't count on international law, then you must find other ways to ensure your security.
[00:07:50.131 --> 00:07:54.395]  That's a growing number of countries seek to acquire weapons of mass destruction.
[00:07:54.715 --> 00:07:55.436]  This is logical.
[00:07:55.736 --> 00:07:57.438]  If you have a bomb, no one will touch you.
[00:07:57.938 --> 00:08:03.221]  We're left with talk of the need to strengthen non-proliferation, when in reality, this is being eroded.
[00:08:03.741 --> 00:08:05.702]  Exactly the opposite of what we're told.
[00:08:05.742 --> 00:08:14.307]  We're told that they've got to go in and take out weapons of mass destruction, but they understand that when Qaddafi gave up his weapons of mass destruction, they came after him.
[00:08:14.347 --> 00:08:15.948]  They understand the only way that they're safe
[00:08:16.388 --> 00:08:16.568]  Is it?
[00:08:38.383 --> 00:08:50.191]  Millions around the world increasingly see America not as a model of democracy, but as relying solely on brute force cobbling together coalitions under the slogan, you're either with us or against us.
[00:08:50.671 --> 00:08:55.334]  But brute force has proved ineffective and pointless, and he gives examples of that.
[00:08:55.834 --> 00:09:05.519]  Now, of course, one reason the Obama administration, or I should say regime, was so anxious to get involved in yet another war was to cover up the scandals here at home.
[00:09:05.899 --> 00:09:07.160]  Remember the IRS scandal?
[00:09:07.560 --> 00:09:08.200]  We certainly do.
[00:09:08.800 --> 00:09:13.703]  And it is much broader, much more severe than anything that Nixon was impeached for.
[00:09:13.803 --> 00:09:17.044]  And he was impeached for using the IRS against political enemies.
[00:09:17.705 --> 00:09:20.266]  This is what Lois Lerner said a few months ago.
[00:09:21.030 --> 00:09:29.916]  Members of this committee have accused me of providing false information when I responded to questions about the IRS processing of applications for tax exemption.
[00:09:32.177 --> 00:09:35.099]  I have not done anything wrong.
[00:09:36.860 --> 00:09:39.021]  Well, eventually the truth comes out.
[00:09:39.281 --> 00:09:41.123]  She said that in hearings in May.
[00:09:42.014 --> 00:09:55.729]  But we learned today that emails in February of 2011 she advised her staff she said counsel and her advisor Judy Connell need to be in on this talking about the Tea Party audits.
[00:09:56.029 --> 00:10:00.474]  She said Cincy should not have these cases that would be Cincinnati.
[00:10:01.345 --> 00:10:10.133]  Now remember in May, the testimony from IRS Commissioner Stephen Miller was saying that this was just the work of two rogue employees in Cincinnati, and Ms.
[00:10:10.173 --> 00:10:18.681]  Lerner suggested that her office had been totally unaware of the pattern of targeting conservative Tea Party groups until she read about it in the newspaper.
[00:10:19.282 --> 00:10:22.225]  Finally, there's this admission in the emails.
[00:10:23.331 --> 00:10:34.394]  A Democrat senatorial campaign committee complained to the FEC that conservative groups like Crossroads, GPS, and Americans for Prosperity should be treated as political committees rather than 501c34s.
[00:10:35.594 --> 00:10:39.815]  She said, perhaps the FEC will save the day, she wrote back.
[00:10:39.995 --> 00:10:40.515]  There you go.
[00:10:40.615 --> 00:10:42.816]  That's the political targeting that they're looking at.
[00:10:43.584 --> 00:10:46.768]  Now it's not just the IRS, it's not just the government.
[00:10:47.149 --> 00:10:56.802]  They have also drawn into their net of falling credibility corporations who are now trying to do a PR campaign to get the public to trust them again.
[00:10:57.600 --> 00:11:05.623]  Both Microsoft and Google came out this week with some kind of face-saving lawsuits that really aren't going to do anything to stop the spying.
[00:11:05.923 --> 00:11:14.666]  All they're going to do is presumably allow them to talk about the fact that they're turning over X number of amount of information to the government per their request.
[00:11:14.726 --> 00:11:20.648]  It's not going to give us any protection at all against illegal corporate spying, but it's meant to be a PR campaign.
[00:11:21.408 --> 00:11:25.569]  And Yahoo and Facebook have been dulged in the same type of PR campaign.
[00:11:26.029 --> 00:11:29.690]  Listen to what Yahoo's CEO Marissa Mayer said.
[00:11:30.390 --> 00:11:40.413]  She said, Yahoo has previously unsuccessfully sued the Foreign Intelligence Surveillance Court, that's FISA, which provides a legal framework for the NSA surveillance.
[00:11:40.993 --> 00:11:42.133]  No, that's wrong.
[00:11:42.693 --> 00:11:46.354]  There is no legal basis for the FISA court.
[00:11:46.914 --> 00:11:48.995]  The FISA court is not a court.
[00:11:49.966 --> 00:11:56.891]  Listen to what John Roberts, Chief Justice of the Supreme Court, said just a couple of months ago at the beginning of July.
[00:11:57.972 --> 00:12:06.838]  When he was informing the Senate of what was going on, it was the Senate Judiciary Committee, he said, when I first learned about the FISA court, I was surprised, he told the committee.
[00:12:07.438 --> 00:12:09.660]  It's not what we usually think of when we think of a court.
[00:12:10.240 --> 00:12:19.762]  We think of a place where we can go, where we can watch, the lawyers argue, and it's subject to the glare of publicity and the judges explain their decision to the public and they can examine them.
[00:12:20.882 --> 00:12:27.684]  Now, why would the Chief Justice of the Supreme Court be surprised about the existence of FISA?
[00:12:28.264 --> 00:12:30.245]  Why would it be news to all the Senators?
[00:12:30.905 --> 00:12:36.566]  Well, because this is something that was done in secret, without any lawful or constitutional authority.
[00:12:36.986 --> 00:12:37.727]  And it's not a court.
[00:12:38.007 --> 00:12:39.567]  Look at the things he's talking about with a court.
[00:12:39.627 --> 00:12:43.068]  He said, in a court, you have a place where you can go and you can watch.
[00:12:43.248 --> 00:12:43.748]  It's public.
[00:12:43.828 --> 00:12:44.529]  It's transparent.
[00:12:44.849 --> 00:12:45.569]  People can see it.
[00:12:45.729 --> 00:12:46.749]  That's the essence of a court.
[00:12:47.289 --> 00:12:48.230]  You also have a jury.
[00:12:48.270 --> 00:12:49.090]  He didn't mention that.
[00:12:49.770 --> 00:12:58.975]  But you do have lawyers arguing and you have the public looking at what's going on and then you have judges who make decisions and explain them to the public and you can see those.
[00:12:59.676 --> 00:13:01.837]  All of that is missing in a FISA court.
[00:13:01.937 --> 00:13:02.577]  It's not a court.
[00:13:03.097 --> 00:13:05.739]  It's a single politically appointed judge.
[00:13:06.579 --> 00:13:09.801]  Now this is what the CEO of Yahoo was saying.
[00:13:10.301 --> 00:13:12.223]  They made their decision and we had to obey them.
[00:13:12.463 --> 00:13:13.263]  Listen to what she says.
[00:13:16.069 --> 00:13:21.173]  We asked to be allowed to publish details of requests they received from the spy agency, the Shahu.
[00:13:21.514 --> 00:13:25.117]  And she said, when you lose and you don't comply, it's treason.
[00:13:26.238 --> 00:13:29.621]  We think it makes more sense to work within the system, she said.
[00:13:30.902 --> 00:13:31.822]  No, it doesn't.
[00:13:32.323 --> 00:13:34.984]  That's the Nuremberg defense, just following orders.
[00:13:35.804 --> 00:13:45.648]  What this is, is a star chamber that is overturning centuries, centuries of a culture of liberty, of a legal foundation, of a framework.
[00:13:46.228 --> 00:13:52.151]  This is as if the government were bombing the Louvre and destroying all the culture of the Louvre.
[00:13:52.751 --> 00:13:56.513]  Or this is as if they were burning the library in Alexandria.
[00:13:57.301 --> 00:14:01.244]  They're destroying everything, going back to the Magna Carta with this.
[00:14:01.685 --> 00:14:04.147]  They're reinstituting things like the Star Chamber.
[00:14:04.387 --> 00:14:06.149]  They're taking away trial by jury.
[00:14:06.329 --> 00:14:08.311]  They're doing all this, they're doing it in secret.
[00:14:08.391 --> 00:14:12.634]  They don't have a court, but they're pretending that they're modifying the Constitution.
[00:14:12.874 --> 00:14:14.656]  They're not modifying the Constitution.
[00:14:14.816 --> 00:14:19.901]  Everything that they're doing has absolutely no authority, no constitutional authority.
[00:14:20.541 --> 00:14:22.422]  They are a regime that is operating.
[00:14:23.183 --> 00:14:30.727]  If she was anything other than a quizling, she would have demanded a real trial, in a real court, with a real jury.
[00:14:31.328 --> 00:14:33.489]  And that jury would have found her innocent.
[00:14:33.789 --> 00:14:35.851]  She would have been a hero, instead of Ed Snowden.
[00:14:36.451 --> 00:14:44.616]  They would have gone behind her, and instead of trying to create some kind of a phony PR, save their face, she would have been a hero.
[00:14:44.736 --> 00:14:48.438]  Or Zuckerberg, if you can imagine him having the guts to stand up to something like that.
[00:14:48.938 --> 00:14:53.441]  But instead, they decided, just like Quisling, that they would go along with the Nazis.
[00:14:54.121 --> 00:14:56.223]  That it was easier for them to do that.
[00:14:56.623 --> 00:14:59.245]  It was easier for them to say that they were just following orders.
[00:14:59.805 --> 00:15:00.966]  So that's where we are today.
[00:15:01.986 --> 00:15:09.508]  And while Rome burns, we've got Democrats and Republicans that are pushing further in other areas of our life, in food.
[00:15:09.828 --> 00:15:22.952]  While we've been trying to constrain our government in other areas, House Republicans have quietly included a three-month extension for the Farmers Assurance Provision in the new spending bill, or as it's popularly known, the Monsanto Protection Act.
[00:15:23.473 --> 00:15:27.774]  The very act that hundreds of thousands of people were protesting back in May.
[00:15:28.308 --> 00:15:29.549]  That's exactly right, David.
[00:15:29.949 --> 00:15:51.221]  So even though hundreds of thousands of Americans, people all over the world, called on their representatives to stop this genetically modified global takeover in its tracks, instead they've gone ahead and extended this special interest loophole that basically nullifies any court orders that are there to protect farmers, the environment, or public health.
[00:15:52.213 --> 00:15:59.466]  And of course that's the law that prevents judges from putting injunctions to delay these genetically modified seeds, even if people are concerned that they're unsafe.
[00:16:00.859 --> 00:16:01.419]  Exactly.
[00:16:01.779 --> 00:16:15.663]  And Monsanto has been arguing that it's really unfair to single out their company in this farmer assurance provision by naming it the Monsanto Protection Act, because they say there's a lot of other agribusinesses that are really supportive of what this is.
[00:16:15.803 --> 00:16:17.084]  And that is a very good point.
[00:16:17.504 --> 00:16:25.346]  Companies like DuPont, who just paid more than $3 million to bolster opposition for genetically modified food labeling,
[00:16:25.666 --> 00:16:37.696]  So in addition to the four and a half million dollar check that was written by Monsanto, opponents of GMO labeling have raised four times as much as those people that are calling for just basic labeling of food.
[00:16:38.257 --> 00:16:46.844]  So people everywhere have expressed anger with genetically modified foods, they're calling on labeling, so why have our representatives not just allowed this to expire?
[00:16:47.304 --> 00:16:50.085]  Well that is because there is a much bigger agenda going on here.
[00:16:50.225 --> 00:16:52.386]  It's called the Trans-Pacific Partnership.
[00:16:52.807 --> 00:16:57.849]  It's a super secret trade pact that's being negotiated absolutely outside of law.
[00:16:58.329 --> 00:17:07.113]  So basically you just have a bunch of White House loyalists running around the world in strong-arming countries into joining up with the TPP and they've dubbed it free trade.
[00:17:08.354 --> 00:17:16.978]  So, in addition to banning GMO labeling, any country that wants to join the TPP has to agree to let genetically modified crops grow in their country.
[00:17:17.578 --> 00:17:21.679]  Secretly, mind you, because there is no labeling for genetically modified foods.
[00:17:21.719 --> 00:17:23.560]  They've already knocked that out.
[00:17:24.200 --> 00:17:32.744]  And any country that doesn't want genetically modified crops in their country will be labeled anti-free trade practices and they could face economic sanctions.
[00:17:33.004 --> 00:17:41.552]  So if you thought the Bilderberg Group was bad and diabolical, the TPP has already met 19 times since they just started meeting in 2009.
[00:17:42.452 --> 00:17:57.345]  For instance, the TPP is going to open up the food market to allow countries who have virtually no enforcement of regulations on pesticides and herbicides, as well as practicing any food safety protocol, to produce food that's going to be sold to the United States and other nations.
[00:17:58.126 --> 00:17:59.967]  We're seeing this with Smithfield Foods.
[00:18:00.048 --> 00:18:08.014]  They are recommending that their shareholders vote for the pork producer to be sold for $4.7 billion to a Chinese company.
[00:18:08.434 --> 00:18:12.117]  Now, if this happens, that will be the largest ever U.S.
[00:18:12.177 --> 00:18:14.839]  company to be sold to a Chinese company.
[00:18:15.399 --> 00:18:18.581]  A Chinese company will be in control of our pork production.
[00:18:18.841 --> 00:18:23.944]  The Chinese knowingly sell us toxic and poisonous toys to our children.
[00:18:24.145 --> 00:18:34.331]  They have known cancer villages which create the food that feeds their population with rice patties with really high levels of cadmium and chromium, cause leukemia and bone and kidney damage.
[00:18:34.551 --> 00:18:38.794]  This is what they knowingly feed their people and they'll be sending it our way.
[00:18:39.514 --> 00:18:40.815]  It is absolutely perverse.
[00:18:40.895 --> 00:18:53.705]  These corporations have no problem putting profit over public health and safety and apparently the public interest by trying to bypass food labeling as well as protect these biotech giants from any litigation.
[00:18:53.725 --> 00:18:55.827]  And how long is that extension?
[00:18:56.547 --> 00:19:00.469]  Well, due to the extension, the bill is set to expire December 15th.
[00:19:00.729 --> 00:19:05.112]  So, that means we really need to step it up, put a lot of pressure on our representatives.
[00:19:05.492 --> 00:19:07.493]  Time to phone in, time to write in.
[00:19:07.974 --> 00:19:13.557]  There's going to be another march against Monsanto on October 12th, which will really help to keep this issue in the public eye.
[00:19:14.017 --> 00:19:25.864]  But also, I think we really need to watch the Trans-Pacific Partnership and what they're doing, because they are moving really swiftly to implement worldwide corporate takeover
[00:19:26.384 --> 00:19:31.810]  Yeah, and we're going to be covering more about the TPP, the Trans-Pacific Partnership, as well as its Atlantic partner, CAFTA, next week.
[00:19:45.805 --> 00:19:52.069]  The Bilderberg meetings and meetings like that are strategic long-term planning for the global elite, for the corporatists.
[00:19:52.570 --> 00:20:00.335]  These secretive treaties are really kind of the grassroots tactical meetings of how they're going to accomplish this New World Order.
[00:20:00.945 --> 00:20:01.726]  Now, stick around.
[00:20:01.826 --> 00:20:08.730]  Right after the break, we're going to be talking to artificial intelligence expert and cybersecurity expert Roman Jampolsky.
[00:20:09.230 --> 00:20:15.494]  And we're going to be talking about the threat posed by DARPA, by robots, and by the rise of the machines.
[00:20:15.814 --> 00:20:16.555]  So stay tuned.
[00:20:41.073 --> 00:20:44.696]  Are we choosing our own destiny or has it been pre-selected for us?
[00:20:44.936 --> 00:20:50.220]  As we've moved through history, every great leader has had to understand the potential of information.
[00:20:50.460 --> 00:20:57.845]  Billions of dollars have been spent privately and publicly looking at how to tap into your psyche.
[00:20:58.185 --> 00:21:07.272]  From compulsory state education to the Hollywood media brainwashing machine, we are kept in perpetual bondage to the ideas that shape our actions.
[00:21:07.492 --> 00:21:14.498]  When somebody obscures that feedback loop between you observing and testing it out and verifying it, they can take total control of your awareness.
[00:21:14.658 --> 00:21:17.540]  All of this is happening so fast, you need to be ahead of the game.
[00:21:17.560 --> 00:21:24.446]  How to engineer the opinion of the American people so that they would not only endorse but demand a war.
[00:21:25.066 --> 00:21:26.067]  Ordered another one!
[00:21:26.087 --> 00:21:27.228]  Another plane just hit!
[00:21:44.994 --> 00:21:47.575]  Well, joining us today is Dr. Roman Yampolsky.
[00:21:47.875 --> 00:21:49.076]  He holds a Ph.D.
[00:21:49.156 --> 00:21:50.376]  in Computer Science and Engineering.
[00:21:50.416 --> 00:22:00.901]  He's an assistant professor of the Engineering School at the University of Louisville, author of over a hundred publications, several books, many, many articles, and here's why we have him here today.
[00:22:01.661 --> 00:22:10.485]  He has areas of expertise, quite a few of them, behavioral biometrics, pattern recognition, genetic algorithms, neural networks, artificial intelligence, and game theory.
[00:22:11.165 --> 00:22:14.527]  And he is an alumnus of the Singularity University.
[00:22:14.567 --> 00:22:20.930]  So we're going to talk to him about all of those things and potentials and dangers for artificial intelligence.
[00:22:21.590 --> 00:22:22.631]  Welcome, Dr. Yampolsky.
[00:22:23.390 --> 00:22:24.271]  Thank you so much for having me.
[00:22:24.851 --> 00:22:25.672]  Thank you for joining us.
[00:22:26.172 --> 00:22:31.856]  Now, I guess the first thing to do would be to try to define for people what artificial intelligence is.
[00:22:32.176 --> 00:22:39.001]  They've seen movies where they see the HAL computer take over the spaceship, or they've seen the rise of robots and Terminators.
[00:22:39.862 --> 00:22:47.808]  On a realistic standpoint, they may be aware of IBM computers playing Chess Masters or Jeopardy games.
[00:22:48.588 --> 00:22:56.496]  But they may not be aware of more subtle hidden areas where artificial intelligence is already embedded in some very critical systems.
[00:22:56.516 --> 00:23:02.442]  So, could you define artificial intelligence for us and give us some examples of how it's in everyday life?
[00:23:03.209 --> 00:23:03.469]  Sure.
[00:23:03.789 --> 00:23:10.833]  AI is defined as making computers able to do things people typically better at.
[00:23:10.933 --> 00:23:18.877]  So something as trivial as a spell checker on your computer at some point was a very ambitious research project in AI.
[00:23:19.377 --> 00:23:25.621]  Now you no longer think of it as being artificial intelligence, but things like that are everywhere.
[00:23:25.841 --> 00:23:30.043]  We are surrounded by useful applications of AI.
[00:23:30.103 --> 00:23:31.964]  Mail sorting facilities, for example.
[00:23:32.443 --> 00:23:36.348]  Or even getting directions on your phones, giving you a routing.
[00:23:36.368 --> 00:23:42.055]  That used to be a very, that was a real tough technical problem for people to solve, the traveling salesman problem.
[00:23:42.215 --> 00:23:44.137]  Absolutely, absolutely.
[00:23:44.197 --> 00:23:45.359]  Google Maps, yeah.
[00:23:45.879 --> 00:23:49.504]  Now, also you would say you'd be involved in control systems, right?
[00:23:49.584 --> 00:23:51.867]  Like automobiles, for example.
[00:23:52.395 --> 00:23:59.099]  Automobiles, but also electric grid, nuclear power plants, satellite navigation, even stock market.
[00:23:59.299 --> 00:24:02.821]  Everything is controlled or largely controlled by AIs now.
[00:24:04.382 --> 00:24:07.324]  Now, you're a veteran of the Singularity University.
[00:24:07.344 --> 00:24:12.547]  Could you tell us the difference between Singularity and AI, artificial intelligence?
[00:24:13.459 --> 00:24:15.260]  Well, singularity is a concept.
[00:24:15.920 --> 00:24:24.484]  It's an idea that at some point in the future, machines will get to a point where they are as intelligent as people and maybe smarter.
[00:24:24.544 --> 00:24:31.148]  And at that point, the progress in technology is accelerating so fast, you can't really predict what's going to happen.
[00:24:31.208 --> 00:24:37.851]  It's just becoming too exponentially quick for the human mind to keep up with.
[00:24:39.173 --> 00:24:44.194]  You know, when I grew up, everybody, most of the futurist predictions were kind of centered on transportation.
[00:24:44.989 --> 00:24:52.112]  Because, you know, in the middle of the 20th century, people had seen radical, radical changes in transportation.
[00:24:52.152 --> 00:24:59.495]  They had seen airplanes begin and then go from, in one generation, go from airplanes from the Wright Brothers to the moon, for example.
[00:24:59.535 --> 00:25:03.196]  So, people's projections were really kind of based on transportation.
[00:25:03.736 --> 00:25:06.317]  I fully expected by this time to have a flying car.
[00:25:06.958 --> 00:25:07.898]  That hasn't happened yet.
[00:25:07.998 --> 00:25:11.739]  So, how would you say the likelihood of these
[00:25:13.863 --> 00:25:19.111]  Rapid advances and most of the futurist predictions today are kind of centered on AI.
[00:25:20.393 --> 00:25:27.505]  How likely would you say that is to happen or do you think that might hit a kind of a leveling that we saw happen in transportation?
[00:25:28.450 --> 00:25:29.952]  Well, it's very possible.
[00:25:30.052 --> 00:25:35.477]  There is a lot of discussion about exactly what's going to happen and how quickly and how soon.
[00:25:36.058 --> 00:25:38.620]  So I'm trying to be conservative in my predictions.
[00:25:38.701 --> 00:25:41.964]  It might take as long as 200 years for us to get to that point.
[00:25:42.364 --> 00:25:43.125]  Will it happen?
[00:25:43.485 --> 00:25:44.386]  It's very likely.
[00:25:45.387 --> 00:25:49.752]  Is there an upper limit to intelligence, upper limit to what those systems are capable of?
[00:25:50.092 --> 00:25:54.976]  Quite possible, but it's still a lot higher than what human beings are capable of.
[00:25:55.416 --> 00:26:00.960]  So for us it still would be very, very impressive of what those machines can do compared to us.
[00:26:01.501 --> 00:26:07.626]  Will it be an instant change, something happening in a matter of minutes or days versus years or decades?
[00:26:08.546 --> 00:26:12.910]  We're not sure about that, but it's equally interesting in both cases.
[00:26:13.939 --> 00:26:17.041]  Let's talk about the dangers of artificial intelligence.
[00:26:18.182 --> 00:26:24.106]  Everybody's pretty much aware because we've talked about it with the Michael Hastings car accident.
[00:26:24.146 --> 00:26:26.247]  We've talked about the possibility of cyber attacks.
[00:26:26.567 --> 00:26:27.708]  Richard Clarke has talked about it.
[00:26:27.808 --> 00:26:28.789]  DARPA's talked about it.
[00:26:29.349 --> 00:26:31.131]  We've seen videos of people doing that.
[00:26:31.711 --> 00:26:34.353]  Those are very simple AI systems, for example.
[00:26:34.413 --> 00:26:36.835]  You've got, let's say, an anti-lock braking system.
[00:26:36.995 --> 00:26:39.718]  That is, at one level, that's artificial intelligence.
[00:26:39.738 --> 00:26:44.962]  Wouldn't you characterize it that way, where it's making some decisions based on inputs and taking corrective action?
[00:26:45.592 --> 00:26:52.679]  For sure, you can explain something as trivial as an if statement in a program as a simple case of AI.
[00:26:52.819 --> 00:27:00.286]  We are more interested in general AI, human-level AI, but all of those are certainly examples of AI systems.
[00:27:00.606 --> 00:27:02.308]  But kind of staying with that analogy,
[00:27:04.255 --> 00:27:07.737]  Antilock brakes in an automobile, you've still got your foot on the brake.
[00:27:08.037 --> 00:27:11.658]  So as long as you're telling it to brake, it's trying to figure out the best way to do it.
[00:27:11.678 --> 00:27:13.839]  But ultimately the human is remaining in control.
[00:27:14.659 --> 00:27:19.982]  But I guess the concern would be different failure scenarios.
[00:27:20.022 --> 00:27:22.383]  Let's say maybe a bug in the system.
[00:27:23.003 --> 00:27:27.425]  Or maybe you've got somebody who hacks into it and does things maliciously.
[00:27:27.565 --> 00:27:33.347]  Or maybe there's a government that makes sure that it has a backdoor to this and does malicious things.
[00:27:34.167 --> 00:27:35.608]  How do we guard against those sorts of things?
[00:27:35.648 --> 00:27:37.910]  Because that's one of your areas of expertise, right?
[00:27:37.931 --> 00:27:38.331]  Security?
[00:27:39.072 --> 00:27:40.333]  Right, and you really can't.
[00:27:40.413 --> 00:27:45.618]  If a designer of a system puts a backdoor in it, there is not much you as a consumer can do about it.
[00:27:47.800 --> 00:27:51.523]  Another area of your expertise is behavioral biometrics.
[00:27:52.564 --> 00:27:56.147]  One of the things that concerns me looking at some of these scenarios is
[00:27:57.441 --> 00:28:13.668]  Using AI to profile people either as common criminals or maybe in a broader sense to profile people and identify them as terrorists and of course that would be a system where bugs could be very very dangerous.
[00:28:13.928 --> 00:28:22.572]  We all remember the movie Brazil where there was a literal bug that fell into a typewriter and altered one character and that set off a chain of events for some innocent people but
[00:28:23.772 --> 00:28:37.869]  A computer that's doing behavioral profiling, if it mistakes somebody that's trying to change their tire for someone who's a car thief, that could set off, in today's environment, that could set off a fatal chain of events for the police, couldn't it?
[00:28:38.579 --> 00:28:48.562]  Absolutely, and we all heard about people being placed on a do-not-fly list, even children, and it's obviously a mistake, a common name, common location, but it can happen to anyone, yeah.
[00:28:49.042 --> 00:28:49.682]  Yeah, yeah.
[00:28:50.142 --> 00:28:54.403]  So, I mean, what types of things do you, how do you guard against that?
[00:28:54.443 --> 00:29:07.387]  I mean, other than just the typical types of things that you would do for software engineering to try to take bugs out, I mean, are there any, when you're looking at control systems and security, is there anything that you would add to it other than just making sure the software works?
[00:29:08.143 --> 00:29:09.704]  You mean as a designer of a system?
[00:29:09.844 --> 00:29:10.124]  Yeah.
[00:29:10.244 --> 00:29:11.184]  As a user of a system?
[00:29:11.445 --> 00:29:18.348]  Well, I mean, you're looking at a system where, let's say, Homeland Security comes in and they want to start doing behavioral profiling for people.
[00:29:18.368 --> 00:29:23.710]  I mean, how do we guard against not only these different failure modes?
[00:29:23.730 --> 00:29:27.292]  I mean, certainly, you know, if the government's going to be malicious, we can't guard against that.
[00:29:27.332 --> 00:29:33.055]  But how do we guard against, say, you know, hackers or against bugs?
[00:29:33.155 --> 00:29:35.596]  And how do we provide some sort of oversight for that?
[00:29:36.155 --> 00:29:38.738]  Well, with hackers, there are state-of-the-art algorithms.
[00:29:38.798 --> 00:29:42.603]  You can use a certain level of encryption, limit network access.
[00:29:42.643 --> 00:29:44.145]  There are better practices.
[00:29:44.205 --> 00:29:49.171]  None of them guarantee security, but you can certainly increase chances of having a safe system.
[00:29:49.652 --> 00:29:52.115]  But the systems like that are inherently
[00:29:52.555 --> 00:29:54.578]  Likely not to be 100% accurate.
[00:29:54.638 --> 00:29:59.663]  At some point you still have to set a threshold for how many false positives you're willing to tolerate.
[00:30:00.044 --> 00:30:11.477]  And if a system is profiling hundreds of millions of citizens, even a tiny percentage, tenth of a percentage point, is still thousands of people who are going to be falsely falsely identified.
[00:30:12.559 --> 00:30:14.159]  Yeah, I guess that's my concern.
[00:30:14.179 --> 00:30:38.024]  I guess, ultimately, and this would just be my opinion and not something that's in your area of expertise, I guess unless we have final human oversight, kind of like, you know, search warrants that have to be looked at by judges, you know, unless we have some kind of legal protection like that, you would say that because we always have this risk of this, we could never really turn these systems fully over into an automated system, wouldn't you say?
[00:30:39.365 --> 00:30:42.087]  Not if you want to remain in control of those systems.
[00:30:42.167 --> 00:30:45.169]  If you turn control over to the system, you are no longer needed.
[00:30:45.209 --> 00:30:49.351]  You become a side product of it and have no decision power.
[00:30:49.971 --> 00:30:51.873]  Well that kind of takes us into the area of ethics.
[00:30:52.693 --> 00:30:56.475]  You've talked about machine ethics that some scientists have proposed.
[00:30:56.495 --> 00:30:56.976]  I guess
[00:30:57.936 --> 00:31:07.867]  One of the things that I've seen with engineers is that too often they ignore or they refuse to really think about the consequences of the things that they're developing.
[00:31:08.027 --> 00:31:10.650]  They're very interested in the technical issues.
[00:31:11.357 --> 00:31:16.661]  It becomes something of a puzzle for them, a mental game for them to solve this puzzle.
[00:31:17.422 --> 00:31:21.706]  And they don't really think about or don't really concern themselves with the ethical things down the road.
[00:31:21.746 --> 00:31:26.049]  Now, people are starting to look at the ethical issues of AI.
[00:31:26.089 --> 00:31:27.711]  You're one of the people who's looking at that.
[00:31:28.411 --> 00:31:39.460]  Some of the people who have looked at these ethical issues have taken the idea that they want to do machine ethics, that they want to give the machine some decision-making capability.
[00:31:39.620 --> 00:31:50.029]  And I guess one of the things, I saw an interesting quote when I looked at this, one of the scientists said this, he said, teaching ethics to a machine will help us to learn what ethics are.
[00:31:51.443 --> 00:32:00.186]  I find that, from my perspective, to be very frightening, because that kind of shows a moral relativism and ethics that, you know, really kind of surprises me.
[00:32:00.247 --> 00:32:02.767]  But what do you think is the right way to do it?
[00:32:02.848 --> 00:32:05.328]  You think that machine ethics are not the correct way to go.
[00:32:05.348 --> 00:32:06.289]  Is that correct?
[00:32:06.369 --> 00:32:10.490]  I am very skeptical of it, mostly because of a quote you brought up there.
[00:32:10.511 --> 00:32:19.274]  We don't know what ethics are, in the sense of, we don't have a universal set of ethics accepted by all people, all cultures, all religions.
[00:32:19.634 --> 00:32:22.958]  We still go to wars over what is an ethical course of action.
[00:32:23.258 --> 00:32:29.445]  To teach machines ethics, we have to first decide which set of ethics are we talking about.
[00:32:29.565 --> 00:32:37.634]  And if that machine is all-powerful and enforcing that set of rules, and everyone else, that can create some very unethical consequences.
[00:32:38.174 --> 00:32:41.278]  So then what is your alternative to machine ethics?
[00:32:42.076 --> 00:32:51.083]  Well, my suggestion was actually not to place machines in a position of power, certainly not in a position where a machine decides which human gets to live or die.
[00:32:51.103 --> 00:33:00.690]  I'm very skeptical of having machines fully, autonomously deciding questions of war, questions of capital punishment.
[00:33:01.510 --> 00:33:23.767]  I don't think there is a hundred percent safe approach just from ethics Point of view right I agree I mean that even goes back to what we were just talking about in terms of even profiling or identifying somebody as a criminal suspect or as a terrorist Giving totally automating that process is a very dangerous thing without human oversight You know people are
[00:33:24.881 --> 00:33:37.204]  People are very much aware of the Terminator series, the Rise of the Machines, where you have machines all of a sudden make this quantum leap in intelligence and achieve parity with humans or even superiority to humans.
[00:33:38.184 --> 00:33:43.825]  But there's another work out there, The Artelect War, which I'm sure you're probably aware of that, right?
[00:33:44.645 --> 00:33:46.846]  That's Hugo de Garris' work.
[00:33:48.227 --> 00:33:49.448]  What do you think is more likely?
[00:33:49.468 --> 00:33:51.770]  Well, let's talk about that initially first, too.
[00:33:51.790 --> 00:33:58.196]  In that, it's kind of more of an evolving scenario, where you have essentially two camps.
[00:33:58.236 --> 00:34:04.202]  You have the technological, political elite, which want to see artificial intelligence
[00:34:05.219 --> 00:34:08.560]  Essentially making another species, according to his book.
[00:34:08.780 --> 00:34:18.984]  And then you have the, they eventually leave the planet because they have the technological means and because the rest of the people, the mass of humanity, are kind of concerned about what's happening.
[00:34:19.264 --> 00:34:23.986]  Those people, the Terrans, are trying to put limits on this technology that the cosmos don't.
[00:34:24.026 --> 00:34:31.629]  It's kind of like the Elysium movie that just came out, where you have this mass of humanity down on the Earth and you have this technological elite up in orbit.
[00:34:32.209 --> 00:34:40.234]  But he envisions a war over this between these technologically elite people and the rest of the people who don't take that scenario.
[00:34:40.454 --> 00:34:49.820]  Do you see something like that happening as being a more likely scenario as opposed to machines suddenly becoming self-aware without anybody doing anything to help them along?
[00:34:51.414 --> 00:34:53.075]  Well, it's a complicated question.
[00:34:53.115 --> 00:34:53.955]  Let's break it up.
[00:34:54.015 --> 00:34:55.356]  So, the artilect war.
[00:34:55.496 --> 00:34:58.517]  I do feel it's very possible that something like that might happen.
[00:34:58.557 --> 00:35:01.318]  It's not necessarily over the issue of AI.
[00:35:01.658 --> 00:35:05.720]  Any scientific question can be just as divisive.
[00:35:06.000 --> 00:35:07.741]  Think stem cell research.
[00:35:07.761 --> 00:35:10.542]  We already have enough people supporting it or opposing it.
[00:35:10.882 --> 00:35:13.063]  Human cloning, things of that nature.
[00:35:13.643 --> 00:35:15.064]  There is no consensus on it.
[00:35:16.284 --> 00:35:21.126]  They're definitely worth going to war over for some people, so that is a possibility.
[00:35:21.166 --> 00:35:22.366]  That's true.
[00:35:22.666 --> 00:35:25.807]  In this book, he focuses simply just on artificial intelligence.
[00:35:25.907 --> 00:35:29.008]  Artelec is an acronym for that, essentially.
[00:35:29.068 --> 00:35:30.829]  But yeah, you're right.
[00:35:30.849 --> 00:35:33.550]  It could be something like genetic engineering.
[00:35:33.670 --> 00:35:43.713]  They might create Chimera, and you might have essentially another race or some kind of new species or danger created out of biological tampering.
[00:35:44.062 --> 00:36:01.899]  Now when we look at the singularity and we look at some of the things that Ray Kurzweil has proposed, I guess my idea, what he comes up with in The Artelect War, not Ray Kurzweil but Hugo de Garis, he's talking about people, the elite, who are essentially trying to create a new race of gods to rule over them, whereas
[00:36:02.560 --> 00:36:07.966]  I think it's a more likely scenario that the elite themselves may see themselves as gods.
[00:36:08.086 --> 00:36:17.177]  Do you see that in any of the... Are you concerned about that in looking at the singularity, looking at what Ray Kurzweil and others are proposing?
[00:36:17.915 --> 00:36:23.378]  Well, Kurzweil is talking about integration of machine and human beings.
[00:36:23.499 --> 00:36:28.642]  We will become one with machines, and that would be our way of adapting to singularity.
[00:36:29.022 --> 00:36:41.270]  The Goddess, in his book, talks about people who do see our mission as leading to creation of better species, a super-intelligent, super-God-like species of artilects.
[00:36:41.890 --> 00:36:45.451]  And for him it's not a big deal if humanity is lost in the process.
[00:36:45.511 --> 00:36:53.793]  It's sort of like previous Neanderthals and other human species died out to give birth to homo sapiens.
[00:36:54.173 --> 00:37:00.115]  To them it's the same process of evolution taking the next step, essentially.
[00:37:01.115 --> 00:37:08.379]  I guess as we look at these different machines, one of the things that concerns me is what Eisenhower said in his departing speech.
[00:37:08.519 --> 00:37:13.461]  Everybody focuses on the military-industrial complex, and I think that certainly is a part of this.
[00:37:14.401 --> 00:37:21.004]  One of the other things that he talked about was the domination of research by the government, particularly by the military.
[00:37:21.044 --> 00:37:29.128]  He said, the prospect of domination of the nation's scholars by federal employment, project allocations, and the power of money is ever-present
[00:37:29.668 --> 00:37:31.069]  And to be regarded gravely.
[00:37:32.190 --> 00:37:34.071]  That's, I guess, my concern about DARPA.
[00:37:34.091 --> 00:37:46.278]  When I start looking at these projects that they have, it seems like, of course, they're coming at it from a military standpoint, but they're creating these robots that can run faster than humans, that can climb, that can have super strength.
[00:37:46.898 --> 00:37:50.380]  It looks like they're trying to replace soldiers with a robotic army.
[00:37:51.120 --> 00:37:53.582]  What do you see as the likelihood of that happening?
[00:37:54.210 --> 00:37:55.110]  Oh, it's very likely.
[00:37:55.150 --> 00:38:06.075]  We already have autonomous drones flying over, spying, even capable of engaging in actual fighting, shooting down targets.
[00:38:06.515 --> 00:38:08.656]  So it's very likely to happen, absolutely.
[00:38:09.976 --> 00:38:19.740]  And do you see that as kind of giving a leverage to a small group of elite to do whatever they wish because now they don't have a pushback of the military?
[00:38:21.311 --> 00:38:26.393]  Well, the main problem is that if you have robots actually engaging in fighting, who is responsible?
[00:38:26.453 --> 00:38:35.536]  You can always say, you know, it wasn't me who killed civilians, it was a system going bad, being hacked, it has a bug.
[00:38:35.917 --> 00:38:40.098]  So you have a lot of issues with deniability of responsibility in case of war.
[00:38:41.339 --> 00:38:44.860]  And again, how would you guard against that as a security expert?
[00:38:44.900 --> 00:38:49.762]  How would you guard against, say, DARPA creating an army of robots?
[00:38:50.691 --> 00:38:51.732]  Well, you really can't.
[00:38:51.852 --> 00:38:56.656]  As a DARPA designer, you can make sure that nobody can hack your robot, or at least it's unlikely.
[00:38:56.696 --> 00:38:58.537]  You can encrypt communication channels.
[00:38:58.557 --> 00:39:03.761]  You can have confirmations to make sure that the memory of the system has not been altered.
[00:39:04.221 --> 00:39:07.264]  But you can't really prevent this from happening.
[00:39:07.324 --> 00:39:10.626]  It's just not something you can do.
[00:39:10.966 --> 00:39:13.949]  You can't stop the amount of funding they have.
[00:39:15.270 --> 00:39:19.473]  You can't deal with it as a civilian or as a foreign nation.
[00:39:19.864 --> 00:39:23.446]  Yeah, I guess that's kind of the age-old question is, who watches the watchers?
[00:39:23.506 --> 00:39:25.727]  Who controls the controllers?
[00:39:27.188 --> 00:39:32.570]  How do we keep control of it when we concentrate so much power into the hands of so few?
[00:39:32.610 --> 00:39:44.396]  And I guess that's the thing that really concerns me and a lot of us is that this technology, these robots, these drones, is really concentrating power in the hands of a few, isn't it?
[00:39:45.366 --> 00:39:47.448]  It does, but my concern is the next step.
[00:39:47.528 --> 00:39:52.072]  If the system is smarter than the guy controlling it, is the system now in charge?
[00:39:52.672 --> 00:39:55.735]  These people may think they're going to be in charge of it, but for how long?
[00:39:56.055 --> 00:40:02.280]  How long before the system becomes completely autonomous, completely independent, and makes its own decisions?
[00:40:03.261 --> 00:40:04.382]  Is that where you see this headed?
[00:40:04.522 --> 00:40:06.804]  Is that your major concern?
[00:40:06.904 --> 00:40:11.128]  Is the machines themselves becoming intelligent enough that they take over?
[00:40:11.641 --> 00:40:12.342]  I would say so.
[00:40:12.382 --> 00:40:15.664]  It concerns me a lot more than any single person.
[00:40:15.684 --> 00:40:20.567]  I mean, people can be taken out, but a system like that, there's not much we can do.
[00:40:22.248 --> 00:40:24.990]  What kind of a time frame do you see of that happening?
[00:40:25.010 --> 00:40:29.713]  So, different people have different predictions.
[00:40:29.893 --> 00:40:35.877]  I'm fairly confident saying something like 2045, sort of in line with what Kurzweil predicts.
[00:40:36.994 --> 00:40:42.557]  Well, let me just kind of turn it over to you and just let you tell us what you think is a solution to this.
[00:40:43.397 --> 00:40:44.718]  That's your area of expertise.
[00:40:45.218 --> 00:40:56.244]  Your concern is that machines become intelligent, that they could... And you're more concerned about that, right, than about humans who are taking over, using concentrated force, concentrated technology.
[00:40:56.564 --> 00:41:05.449]  You're more concerned of the... You're more concerned about the machines becoming intelligent and taking things over than a small cadre of corrupt humans.
[00:41:05.549 --> 00:41:05.989]  Is that correct?
[00:41:06.884 --> 00:41:07.824]  I would say so, yes.
[00:41:09.205 --> 00:41:14.606]  My concern is basically that we will create something we can no longer control.
[00:41:15.006 --> 00:41:19.807]  There is no way to undo this process, and as far as I know, no one has a solution to it.
[00:41:19.847 --> 00:41:21.408]  You keep asking me what should we do.
[00:41:21.428 --> 00:41:23.208]  I don't think there is a solution.
[00:41:23.268 --> 00:41:34.151]  If we get to a point where we create a robot or program which is more intelligent than we are, and it is released, it's available to others on the Internet or by other means,
[00:41:34.591 --> 00:41:36.233]  There is nothing you can do to undo it.
[00:41:36.393 --> 00:41:40.677]  I mean, it's like asking, well, what can you do to stop electricity from being used?
[00:41:40.978 --> 00:41:41.298]  Nothing.
[00:41:43.020 --> 00:41:47.745]  And you would not advocate a kind of a Luddite approach where we just shut down technology.
[00:41:48.545 --> 00:41:53.110]  You're advocating essentially, you know, technology can be used for both good and for evil.
[00:41:53.170 --> 00:41:56.554]  You would advocate that we try to outpace this or?
[00:41:58.206 --> 00:42:00.547]  Well, again, I don't have a perfect solution.
[00:42:00.807 --> 00:42:14.150]  I can tell you what others have proposed, so Ray Kurzweil suggests that the solution is to merge with technology, become a machine, have brain implants, and try to keep up with improvements this way.
[00:42:14.570 --> 00:42:20.892]  My concern is that in the process we stop being human as well, we become machines, so it's not really a perfect solution.
[00:42:20.912 --> 00:42:26.413]  I would be more careful in trying to develop those systems,
[00:42:26.733 --> 00:42:35.999]  We do have restrictions on certain types of research, chemical weapons, bio weapons, nuclear weapons, even stem cell research, human cloning.
[00:42:36.039 --> 00:42:38.821]  We're limiting those technologies and for good reason.
[00:42:39.261 --> 00:42:52.710]  I don't see why we can't have similar programs for artificial intelligence where you have to go through review boards, you have to get approval from ethics boards before you get millions of dollars to develop an AI system, especially a military one.
[00:42:53.090 --> 00:42:54.371]  Oversight committees, essentially.
[00:42:54.771 --> 00:43:00.594]  And that's where a real problem is, is because in America, we don't have an oversight committee.
[00:43:00.614 --> 00:43:03.616]  What we have is DARPA, which essentially operates in secret.
[00:43:04.016 --> 00:43:15.283]  You know, like the NSA, like so many other government agencies, there's not only is there no oversight, but we don't really know what their agenda is, other than to create more and more powerful weapons.
[00:43:15.987 --> 00:43:18.689]  Well, their concern is to outpace competition.
[00:43:18.750 --> 00:43:27.998]  They're worried that the Chinese or Russians will get there first, and for them it's more important to get there first than to do it slowly, ethically, and do it right.
[00:43:28.338 --> 00:43:29.760]  So they have their priorities.
[00:43:30.200 --> 00:43:30.600]  That's right.
[00:43:31.801 --> 00:43:34.264]  Well, thank you so much for joining us today, Dr. Yampolsky.
[00:43:34.926 --> 00:43:43.191]  We really appreciate your insights and your expertise, and I hope that you are successful in containing what we're all concerned about.
[00:43:43.792 --> 00:43:54.718]  I hope that perhaps we can have more human oversight of where some of this research is headed, take a longer look at the implications and the ethics of what we're doing.
[00:43:54.979 --> 00:43:55.399]  What do you say?
[00:43:56.055 --> 00:43:59.139]  Thank you so much for having me, and yeah, the work is just beginning.
[00:43:59.199 --> 00:44:00.902]  It's a very hot area of research.
[00:44:00.942 --> 00:44:07.671]  I hope more people get interested in this, provide support, logistics support, financial support.
[00:44:08.192 --> 00:44:11.376]  I think it's a very important area of investigation.
[00:44:12.057 --> 00:44:12.418]  Very good.
[00:44:12.458 --> 00:44:13.038]  Thank you very much.
[00:44:14.161 --> 00:44:35.270]  Well, as Dr. Yampolsky said, the greatest danger that something may fail in one of these failure scenarios is that we may have an elite that can do whatever they wish without any pushback from a human military, or that some of these technologies themselves might get out of control, whether it's artificial intelligence or whether it's some kind of genetic splicing, modification.
[00:44:36.485 --> 00:44:44.888]  The greatest danger is that the populace would be ignorant about these dangers, that we would be complacent, that we would not do anything about that.
[00:44:45.229 --> 00:44:48.390]  One of the ways to fight that is to educate yourself, to educate others.
[00:44:48.730 --> 00:44:57.073]  The very first magazine we had, the very first InfoWars magazine, was about Rise of the Robots, and the one that just came out is about government acting as God.
[00:44:57.253 --> 00:45:00.675]  That kind of bookends both of the greatest dangers from the AI community.
[00:45:01.315 --> 00:45:06.702]  We also have a book that we sell here at the InfoWars store, InfoWarsStore.com, Robot Alchemy.
[00:45:06.722 --> 00:45:12.929]  It's by Tex Meyers and he talks about androids, cyborgs, and the magic of artificial life.
[00:45:14.039 --> 00:45:16.200]  That's available at InfoWarsStore.com.
[00:45:16.300 --> 00:45:23.602]  And of course, another way to educate people is to become a subscriber, if you're not, to Prison Planet TV and to allow other people to view that.
[00:45:23.662 --> 00:45:26.443]  Up to 10 people can view that at the same time along with you.
[00:45:26.983 --> 00:45:27.844]  Well, that's it for tonight.
[00:45:27.864 --> 00:45:29.304]  We'll be back at 7 p.m.
[00:45:29.344 --> 00:45:30.625]  Central tomorrow, 8 p.m.
[00:45:30.745 --> 00:45:31.045]  Eastern.
[00:45:37.344 --> 00:45:42.465]  Now you can watch Alex Jones live at InfoWars.com forward slash show.
[00:45:42.625 --> 00:45:47.747]  You'll find links to all of our content there and a free 15-day trial for Prison Planet TV.
[00:45:48.127 --> 00:45:56.009]  You can also browse the network, the InfoWars Nightly News, and over 60 movies and documentaries all together in one place.
[00:45:56.229 --> 00:45:59.210]  You can watch the Alex Jones Radio Show live as it happens.
[00:45:59.570 --> 00:46:02.871]  So check it out, InfoWars.com forward slash show.
